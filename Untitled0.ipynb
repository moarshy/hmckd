{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"colab":{"name":"Untitled0.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"6l4Um-t6EvwF"},"source":["# default_exp saint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjZlNWuvEvwE"},"source":["#hide\n","!pip3 install nbdev -q\n","!pip install einops -q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zGhnBkxEvwE"},"source":["#hide\n","from nbdev.showdoc import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S1yMyNowEvwG"},"source":["#exporti\n","import torch\n","import torch.nn.functional as F\n","from torch import nn, einsum\n","import numpy as np\n","from einops import rearrange\n","\n","\n","def exists(val):\n","    return val is not None\n","\n","def default(val, d):\n","    return val if exists(val) else d\n","\n","def ff_encodings(x,B):\n","    x_proj = (2. * np.pi * x.unsqueeze(-1)) @ B.t()\n","    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n","\n","# classes\n","\n","class Residual(nn.Module):\n","    def __init__(self, fn):\n","        super().__init__()\n","        self.fn = fn\n","\n","    def forward(self, x, **kwargs):\n","        return self.fn(x, **kwargs) + x\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","\n","    def forward(self, x, **kwargs):\n","        return self.fn(self.norm(x), **kwargs)\n","\n","# attention\n","\n","class GEGLU(nn.Module):\n","    def forward(self, x):\n","        x, gates = x.chunk(2, dim = -1)\n","        return x * F.gelu(gates)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, mult = 4, dropout = 0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, dim * mult * 2),\n","            GEGLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(dim * mult, dim)\n","        )\n","\n","    def forward(self, x, **kwargs):\n","        return self.net(x)\n","\n","class Attention(nn.Module):\n","    def __init__(\n","        self,\n","        dim,\n","        heads = 8,\n","        dim_head = 16,\n","        dropout = 0.\n","    ):\n","        super().__init__()\n","        inner_dim = dim_head * heads\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n","        self.to_out = nn.Linear(inner_dim, dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        h = self.heads\n","        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n","        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n","        attn = sim.softmax(dim = -1)\n","        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n","        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n","        return self.to_out(out)\n","\n","\n","class RowColTransformer(nn.Module):\n","    def __init__(self, num_tokens, dim, nfeats, depth, heads, dim_head, attn_dropout, ff_dropout,style='col'):\n","        super().__init__()\n","        self.embeds = nn.Embedding(num_tokens, dim)\n","        self.layers = nn.ModuleList([])\n","        self.mask_embed =  nn.Embedding(nfeats, dim)\n","        self.style = style\n","        for _ in range(depth):\n","            if self.style == 'colrow':\n","                self.layers.append(nn.ModuleList([\n","                    PreNorm(dim, Residual(Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout))),\n","                    PreNorm(dim, Residual(FeedForward(dim, dropout = ff_dropout))),\n","                    PreNorm(dim*nfeats, Residual(Attention(dim*nfeats, heads = heads, dim_head = 64, dropout = attn_dropout))),\n","                    PreNorm(dim*nfeats, Residual(FeedForward(dim*nfeats, dropout = ff_dropout))),\n","                ]))\n","            else:\n","                self.layers.append(nn.ModuleList([\n","                    PreNorm(dim*nfeats, Residual(Attention(dim*nfeats, heads = heads, dim_head = 64, dropout = attn_dropout))),\n","                    PreNorm(dim*nfeats, Residual(FeedForward(dim*nfeats, dropout = ff_dropout))),\n","                ]))\n","\n","    def forward(self, x, x_cont=None, mask = None):\n","        if x_cont is not None:\n","            x = torch.cat((x,x_cont),dim=1)\n","        _, n, _ = x.shape\n","        if self.style == 'colrow':\n","            for attn1, ff1, attn2, ff2 in self.layers: \n","                x = attn1(x)\n","                x = ff1(x)\n","                x = rearrange(x, 'b n d -> 1 b (n d)')\n","                x = attn2(x)\n","                x = ff2(x)\n","                x = rearrange(x, '1 b (n d) -> b n d', n = n)\n","        else:\n","             for attn1, ff1 in self.layers:\n","                x = rearrange(x, 'b n d -> 1 b (n d)')\n","                x = attn1(x)\n","                x = ff1(x)\n","                x = rearrange(x, '1 b (n d) -> b n d', n = n)\n","        return x\n","\n","\n","# transformer\n","class Transformer(nn.Module):\n","    def __init__(self, num_tokens, dim, depth, heads, dim_head, attn_dropout, ff_dropout):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","\n","\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, Residual(Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout))),\n","                PreNorm(dim, Residual(FeedForward(dim, dropout = ff_dropout))),\n","            ]))\n","\n","    def forward(self, x, x_cont=None):\n","        if x_cont is not None:\n","            x = torch.cat((x,x_cont),dim=1)\n","        for attn, ff in self.layers:\n","            x = attn(x)\n","            x = ff(x)\n","        return x\n","    \n","\n","#mlp\n","class MLP(nn.Module):\n","    def __init__(self, dims, act = None):\n","        super().__init__()\n","        dims_pairs = list(zip(dims[:-1], dims[1:]))\n","        layers = []\n","        for ind, (dim_in, dim_out) in enumerate(dims_pairs):\n","            is_last = ind >= (len(dims) - 1)\n","            linear = nn.Linear(dim_in, dim_out)\n","            layers.append(linear)\n","\n","            if is_last:\n","                continue\n","            if act is not None:\n","                layers.append(act)\n","\n","        self.mlp = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.mlp(x)\n","\n","class simple_MLP(nn.Module):\n","    def __init__(self,dims):\n","        super(simple_MLP, self).__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(dims[0], dims[1]),\n","            nn.ReLU(),\n","            nn.Linear(dims[1], dims[2])\n","        )\n","        \n","    def forward(self, x):\n","        if len(x.shape)==1:\n","            x = x.view(x.size(0), -1)\n","        x = self.layers(x)\n","        return x\n","\n","# main class\n","\n","class TabAttention(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        categories,\n","        num_continuous,\n","        dim,\n","        depth,\n","        heads,\n","        dim_head = 16,\n","        dim_out = 1,\n","        mlp_hidden_mults = (4, 2),\n","        mlp_act = None,\n","        num_special_tokens = 1,\n","        continuous_mean_std = None,\n","        attn_dropout = 0.,\n","        ff_dropout = 0.,\n","        lastmlp_dropout = 0.,\n","        cont_embeddings = 'MLP',\n","        scalingfactor = 10,\n","        attentiontype = 'col'\n","    ):\n","        super().__init__()\n","        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\n","\n","        # categories related calculations\n","        self.num_categories = len(categories)\n","        self.num_unique_categories = sum(categories)\n","\n","        # create category embeddings table\n","\n","        self.num_special_tokens = num_special_tokens\n","        self.total_tokens = self.num_unique_categories + num_special_tokens\n","\n","        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n","        categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = num_special_tokens)\n","        categories_offset = categories_offset.cumsum(dim = -1)[:-1]\n","        \n","        self.register_buffer('categories_offset', categories_offset)\n","\n","\n","        self.norm = nn.LayerNorm(num_continuous)\n","        self.num_continuous = num_continuous\n","        self.dim = dim\n","        self.cont_embeddings = cont_embeddings\n","        self.attentiontype = attentiontype\n","\n","        if self.cont_embeddings == 'MLP':\n","            self.simple_MLP = nn.ModuleList([simple_MLP([1,100,self.dim]) for _ in range(self.num_continuous)])\n","            input_size = (dim * self.num_categories)  + (dim * num_continuous)\n","            nfeats = self.num_categories + num_continuous\n","        else:\n","            print('Continous features are not passed through attention')\n","            input_size = (dim * self.num_categories) + num_continuous\n","            nfeats = self.num_categories \n","\n","        # transformer\n","        if attentiontype == 'col':\n","            self.transformer = Transformer(\n","                num_tokens = self.total_tokens,\n","                dim = dim,\n","                depth = depth,\n","                heads = heads,\n","                dim_head = dim_head,\n","                attn_dropout = attn_dropout,\n","                ff_dropout = ff_dropout\n","            )\n","        elif attentiontype in ['row','colrow'] :\n","            self.transformer = RowColTransformer(\n","                num_tokens = self.total_tokens,\n","                dim = dim,\n","                nfeats= nfeats,\n","                depth = depth,\n","                heads = heads,\n","                dim_head = dim_head,\n","                attn_dropout = attn_dropout,\n","                ff_dropout = ff_dropout,\n","                style = attentiontype\n","            )\n","\n","        l = input_size // 8\n","        hidden_dimensions = list(map(lambda t: l * t, mlp_hidden_mults))\n","        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n","        \n","        self.mlp = MLP(all_dimensions, act = mlp_act)\n","        self.embeds = nn.Embedding(self.total_tokens, self.dim) #.to(device)\n","\n","        cat_mask_offset = F.pad(torch.Tensor(self.num_categories).fill_(2).type(torch.int8), (1, 0), value = 0) \n","        cat_mask_offset = cat_mask_offset.cumsum(dim = -1)[:-1]\n","\n","        con_mask_offset = F.pad(torch.Tensor(self.num_continuous).fill_(2).type(torch.int8), (1, 0), value = 0) \n","        con_mask_offset = con_mask_offset.cumsum(dim = -1)[:-1]\n","\n","        self.register_buffer('cat_mask_offset', cat_mask_offset)\n","        self.register_buffer('con_mask_offset', con_mask_offset)\n","\n","        self.mask_embeds_cat = nn.Embedding(self.num_categories*2, self.dim)\n","        self.mask_embeds_cont = nn.Embedding(self.num_continuous*2, self.dim)\n","\n","    def forward(self, x_categ, x_cont,x_categ_enc,x_cont_enc):\n","        device = x_categ.device\n","        if self.attentiontype == 'justmlp':\n","            if x_categ.shape[-1] > 0:\n","                flat_categ = x_categ.flatten(1).to(device)\n","                x = torch.cat((flat_categ, x_cont.flatten(1).to(device)), dim = -1)\n","            else:\n","                x = x_cont.clone()\n","        else:\n","            if self.cont_embeddings == 'MLP':\n","                x = self.transformer(x_categ_enc,x_cont_enc.to(device))\n","            else:\n","                if x_categ.shape[-1] <= 0:\n","                    x = x_cont.clone()\n","                else: \n","                    flat_categ = self.transformer(x_categ_enc).flatten(1)\n","                    x = torch.cat((flat_categ, x_cont), dim = -1)                    \n","        flat_x = x.flatten(1)\n","        return self.mlp(flat_x)\n","\n","\n","class sep_MLP(nn.Module):\n","    def __init__(self,dim,len_feats,categories):\n","        super(sep_MLP, self).__init__()\n","        self.len_feats = len_feats\n","        self.layers = nn.ModuleList([])\n","        for i in range(len_feats):\n","            self.layers.append(simple_MLP([dim,5*dim, categories[i]]))\n","\n","        \n","    def forward(self, x):\n","        y_pred = list([])\n","        for i in range(self.len_feats):\n","            x_i = x[:,i,:]\n","            pred = self.layers[i](x_i)\n","            y_pred.append(pred)\n","        return y_pred\n","\n","class SAINT(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        categories,\n","        num_continuous,\n","        dim,\n","        depth,\n","        heads,\n","        dim_head = 16,\n","        dim_out = 1,\n","        mlp_hidden_mults = (4, 2),\n","        mlp_act = None,\n","        num_special_tokens = 0,\n","        continuous_mean_std = None,\n","        attn_dropout = 0.,\n","        ff_dropout = 0.,\n","        cont_embeddings = 'MLP',\n","        scalingfactor = 10,\n","        attentiontype = 'col',\n","        final_mlp_style = 'common',\n","        y_dim = 2\n","        ):\n","        super().__init__()\n","        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\n","\n","        # categories related calculations\n","\n","        self.num_categories = len(categories)\n","        self.num_unique_categories = sum(categories)\n","\n","        # create category embeddings table\n","\n","        self.num_special_tokens = num_special_tokens\n","        self.total_tokens = self.num_unique_categories + num_special_tokens\n","\n","        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n","\n","        categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = num_special_tokens)\n","        categories_offset = categories_offset.cumsum(dim = -1)[:-1]\n","        \n","        self.register_buffer('categories_offset', categories_offset)\n","\n","\n","        self.norm = nn.LayerNorm(num_continuous)\n","        self.num_continuous = num_continuous\n","        self.dim = dim\n","        self.cont_embeddings = cont_embeddings\n","        self.attentiontype = attentiontype\n","        self.final_mlp_style = final_mlp_style\n","\n","        if self.cont_embeddings == 'MLP':\n","            self.simple_MLP = nn.ModuleList([simple_MLP([1,100,self.dim]) for _ in range(self.num_continuous)])\n","            input_size = (dim * self.num_categories)  + (dim * num_continuous)\n","            nfeats = self.num_categories + num_continuous\n","        elif self.cont_embeddings == 'pos_singleMLP':\n","            self.simple_MLP = nn.ModuleList([simple_MLP([1,100,self.dim]) for _ in range(1)])\n","            input_size = (dim * self.num_categories)  + (dim * num_continuous)\n","            nfeats = self.num_categories + num_continuous\n","        else:\n","            print('Continous features are not passed through attention')\n","            input_size = (dim * self.num_categories) + num_continuous\n","            nfeats = self.num_categories \n","\n","        # transformer\n","        if attentiontype == 'col':\n","            self.transformer = Transformer(\n","                num_tokens = self.total_tokens,\n","                dim = dim,\n","                depth = depth,\n","                heads = heads,\n","                dim_head = dim_head,\n","                attn_dropout = attn_dropout,\n","                ff_dropout = ff_dropout\n","            )\n","        elif attentiontype in ['row','colrow'] :\n","            self.transformer = RowColTransformer(\n","                num_tokens = self.total_tokens,\n","                dim = dim,\n","                nfeats= nfeats,\n","                depth = depth,\n","                heads = heads,\n","                dim_head = dim_head,\n","                attn_dropout = attn_dropout,\n","                ff_dropout = ff_dropout,\n","                style = attentiontype\n","            )\n","\n","        l = input_size // 8\n","        hidden_dimensions = list(map(lambda t: l * t, mlp_hidden_mults))\n","        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n","        \n","        self.mlp = MLP(all_dimensions, act = mlp_act)\n","        self.embeds = nn.Embedding(self.total_tokens, self.dim) #.to(device)\n","\n","        cat_mask_offset = F.pad(torch.Tensor(self.num_categories).fill_(2).type(torch.int8), (1, 0), value = 0) \n","        cat_mask_offset = cat_mask_offset.cumsum(dim = -1)[:-1]\n","\n","        con_mask_offset = F.pad(torch.Tensor(self.num_continuous).fill_(2).type(torch.int8), (1, 0), value = 0) \n","        con_mask_offset = con_mask_offset.cumsum(dim = -1)[:-1]\n","\n","        self.register_buffer('cat_mask_offset', cat_mask_offset)\n","        self.register_buffer('con_mask_offset', con_mask_offset)\n","\n","        self.mask_embeds_cat = nn.Embedding(self.num_categories*2, self.dim)\n","        self.mask_embeds_cont = nn.Embedding(self.num_continuous*2, self.dim)\n","        self.single_mask = nn.Embedding(2, self.dim)\n","        self.pos_encodings = nn.Embedding(self.num_categories+ self.num_continuous, self.dim)\n","        \n","        if self.final_mlp_style == 'common':\n","            self.mlp1 = simple_MLP([dim,(self.total_tokens)*2, self.total_tokens])\n","            self.mlp2 = simple_MLP([dim ,(self.num_continuous), 1])\n","\n","        else:\n","            self.mlp1 = sep_MLP(dim,self.num_categories,categories)\n","            self.mlp2 = sep_MLP(dim,self.num_continuous,np.ones(self.num_continuous).astype(int))\n","\n","\n","        self.mlpfory = simple_MLP([dim ,1000, y_dim])\n","        self.pt_mlp = simple_MLP([dim*(self.num_continuous+self.num_categories) ,6*dim*(self.num_continuous+self.num_categories)//5, dim*(self.num_continuous+self.num_categories)//2])\n","        self.pt_mlp2 = simple_MLP([dim*(self.num_continuous+self.num_categories) ,6*dim*(self.num_continuous+self.num_categories)//5, dim*(self.num_continuous+self.num_categories)//2])\n","\n","        \n","    def forward(self, x_categ, x_cont):\n","        \n","        x = self.transformer(x_categ, x_cont)\n","        cat_outs = self.mlp1(x[:,:self.num_categories,:])\n","        con_outs = self.mlp2(x[:,self.num_categories:,:])\n","        return cat_outs, con_outs "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ir3Uf8OCEvwS"},"source":[""],"execution_count":null,"outputs":[]}]}